{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 01: Import Data/EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the training data\n",
    "\n",
    "# the data set includes four columns: PhraseId, SentenceId, Phrase, Sentiment\n",
    "# In this data set a sentence is further split into phrases \n",
    "# in order to build a sentiment classification model\n",
    "# that can not only predict sentiment of sentences but also shorter phrases\n",
    "\n",
    "# A data example:\n",
    "# PhraseId SentenceId Phrase Sentiment\n",
    "# 1 1 A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .1\n",
    "\n",
    "# the Phrase column includes the training examples\n",
    "# the Sentiment column includes the training labels\n",
    "# \"0\" for very negative\n",
    "# \"1\" for negative\n",
    "# \"2\" for neutral\n",
    "# \"3\" for positive\n",
    "# \"4\" for very positive\n",
    "\n",
    "import numpy as np\n",
    "import pandas as p\n",
    "train=p.read_csv(\"D:\\Darrell\\Desktop\\Syracuse\\Summer_19\\IST_736_Text_Mining\\Week06\\kaggle-sentiment\\kaggle-sentiment/train.tsv\", delimiter='\\t')\n",
    "y=train['Sentiment'].values\n",
    "X=train['Phrase'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93636,) (93636,) (62424,) (62424,)\n",
      "almost in a class with that of Wilde\n",
      "3\n",
      "escape movie\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# check the sklearn documentation for train_test_split\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "# \"test_size\" : float, int, None, optional\n",
    "# If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
    "# If int, represents the absolute number of test samples. \n",
    "# If None, the value is set to the complement of the train size. \n",
    "# By default, the value is set to 0.25. The default will change in version 0.21. It will remain 0.25 only if train_size is unspecified, otherwise it will complement the specified train_size.    \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(X_train[0]) # printing out 1st example in X_train\n",
    "print(y_train[0])\n",
    "print(X_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     1     2     3     4]\n",
      " [ 4141 16449 47718 19859  5469]]\n"
     ]
    }
   ],
   "source": [
    "# Check how many training examples in each category\n",
    "# this is important to see whether the data set is balanced or skewed\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(np.asarray((unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     1     2     3     4]\n",
      " [ 2931 10824 31864 13068  3737]]\n"
     ]
    }
   ],
   "source": [
    "# Print out the category distribution in the test data set. \n",
    "#Is the test data set's category distribution similar to the training data set's?\n",
    "\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(np.asarray((unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 01: Build a unigram MNB and SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn contains two vectorizers\n",
    "\n",
    "# CountVectorizer can give you Boolean or TF vectors\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "# TfidfVectorizer can give you TF or TFIDF vectors\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "# Read the sklearn documentation to understand all vectorization options\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# several commonly used vectorizer setting\n",
    "\n",
    "#  unigram boolean vectorizer, set minimum document frequency to 5\n",
    "# generic encoding is utf8, for this exercise to capture all words/tokens latin-1 was chosen\n",
    "unigram_bool_vectorizer = CountVectorizer(encoding='latin-1', binary=True, min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram term frequency vectorizer, set minimum document frequency to 5\n",
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram and bigram term frequency vectorizer, set minimum document frequency to 5\n",
    "gram12_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,2), min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram tfidf vectorizer, set minimum document frequency to 5\n",
    "unigram_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, min_df=5, stop_words='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93636, 11967)\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "11967\n",
      "[('class', 1858), ('wilde', 11742), ('derring', 2802), ('chilling', 1764), ('affecting', 313), ('meanspirited', 6557), ('personal', 7662), ('low', 6296), ('involved', 5602), ('worth', 11868)]\n"
     ]
    }
   ],
   "source": [
    "# The vectorizer can do \"fit\" and \"transform\"\n",
    "# fit is a process to collect unique tokens into the vocabulary\n",
    "# transform is a process to convert each document to vector based on the vocabulary\n",
    "# These two processes can be done together using fit_transform(), or used individually: fit() or transform()\n",
    "\n",
    "# fit vocabulary in training documents and transform the training documents into vectors\n",
    "X_train_vec = unigram_count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# check the content of a document vector\n",
    "print(X_train_vec.shape)\n",
    "print(X_train_vec[0].toarray())\n",
    "\n",
    "# check the size of the constructed vocabulary\n",
    "print(len(unigram_count_vectorizer.vocabulary_))\n",
    "\n",
    "# print out the first 10 items in the vocabulary\n",
    "print(list(unigram_count_vectorizer.vocabulary_.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62424, 11967)\n"
     ]
    }
   ],
   "source": [
    "# use the vocabulary constructed from the training data to vectorize the test data. \n",
    "# Therefore, use \"transform\" only, not \"fit_transform\", \n",
    "# otherwise \"fit\" would generate a new vocabulary from the test data\n",
    "# any vocabulary NOT in the training data will be ignored in the testing data\n",
    "\n",
    "X_test_vec = unigram_count_vectorizer.transform(X_test)\n",
    "\n",
    "# print out #examples and #features in the test set\n",
    "print(X_test_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93636, 11967)\n",
      "(62424, 11967)\n"
     ]
    }
   ],
   "source": [
    "# Your code starts here\n",
    "unigram_bool_vectorizer1 = CountVectorizer(encoding='latin-1', binary=True, min_df=5, stop_words='english')\n",
    "\n",
    "X_train_vec1 = unigram_bool_vectorizer1.fit_transform(X_train)\n",
    "X_test_vec1 = unigram_bool_vectorizer1.transform(X_test)\n",
    "\n",
    "# check the content of a document vector\n",
    "print(X_train_vec1.shape)\n",
    "print(X_test_vec1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the MNB module\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# initialize the MNB model\n",
    "nb_clf= MultinomialNB()\n",
    "\n",
    "# use the training data to train the MNB model\n",
    "nb_clf.fit(X_train_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most negative words in MNB:\n",
      " [(1.5210602737939296, 'waste'), (1.523562078883295, 'minutes'), (1.533975977581715, 'poorly'), (1.5402375494766518, 'awful'), (1.5463767818167982, 'contrived'), (1.5463767818167982, 'unfunny'), (1.5698200666534547, 'worse'), (1.5969581857419417, 'stupid'), (1.7858251634181175, 'worst'), (1.857793692114055, 'bad')]\n",
      "Top 10 most positive words in MNB:\n",
      " [(0.5958791031947216, 'moving'), (0.604612353536404, 'beautiful'), (0.6156827739590933, 'beautifully'), (0.6217403853248004, 'powerful'), (0.6249198570600717, 'solid'), (0.6298970899440408, 'touching'), (0.6333656656276786, 'gorgeous'), (0.6369651614685079, 'excellent'), (0.6420878254266654, 'best'), (0.6528979981733687, 'wonderful')]\n"
     ]
    }
   ],
   "source": [
    "# Most negative/positive words in the MNB model\n",
    "feature_ranks1 = sorted(zip(nb_clf.feature_log_prob_[4] / nb_clf.feature_log_prob_[0], unigram_count_vectorizer.get_feature_names()))\n",
    "very_negative_features1 = feature_ranks1[-10:]\n",
    "print('Top 10 most negative words in MNB:\\n',very_negative_features1)\n",
    "\n",
    "very_positive_features1 = feature_ranks1[:10]\n",
    "print('Top 10 most positive words in MNB:\\n', very_positive_features1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  742  1276   797   105    11]\n",
      " [  614  4126  5397   655    32]\n",
      " [  248  2385 25756  3239   236]\n",
      " [   19   456  5570  6253   770]\n",
      " [    1    53   729  1977   977]]\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix (row: ground truth; col: prediction)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = nb_clf.fit(X_train_vec, y_train).predict(X_test_vec)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[0,1,2,3,4])\n",
    "print(cm)\n",
    "# first row/column is very negative\n",
    "# last row/column is very positive\n",
    "#row is predicted (x-axis)\n",
    "#column is actual (y-axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45689655 0.49734812 0.67337708 0.51132554 0.482231  ]\n",
      "[0.25315592 0.38118995 0.80831032 0.47849709 0.26143966]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.25      0.33      2931\n",
      "           1       0.50      0.38      0.43     10824\n",
      "           2       0.67      0.81      0.73     31864\n",
      "           3       0.51      0.48      0.49     13068\n",
      "           4       0.48      0.26      0.34      3737\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     62424\n",
      "   macro avg       0.52      0.44      0.47     62424\n",
      "weighted avg       0.59      0.61      0.59     62424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification report\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print(precision_score(y_test, y_pred, average=None))\n",
    "print(recall_score(y_test, y_pred, average=None))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['0','1','2','3','4']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the LinearSVC module\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# initialize the LinearSVC model\n",
    "svm_clf = LinearSVC(C=1)\n",
    "\n",
    "# use the training data to train the model\n",
    "svm_clf.fit(X_train_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Very negative words in SVM (SVC) model\n",
      "(1.6216100498637946, 'cesspool')\n",
      "(1.6484881169807253, 'disappointment')\n",
      "(1.6592495317420688, 'pompous')\n",
      "(1.6683696811106015, 'stinks')\n",
      "(1.692774017797078, 'distasteful')\n",
      "(1.6955904814661282, 'unwatchable')\n",
      "(1.7526397947043106, 'unbearable')\n",
      "(1.7873567368832495, 'stinker')\n",
      "(1.8228705762137276, 'disgusting')\n",
      "(1.823305541733355, 'worthless')\n",
      "\n",
      "Top Very positive words in SVM (SVC) model\n",
      "(1.5635285560162435, 'stunning')\n",
      "(1.6005795112206929, 'astonish')\n",
      "(1.6108129117317336, 'refreshes')\n",
      "(1.6148904549660266, 'flawless')\n",
      "(1.6474646629644183, 'phenomenal')\n",
      "(1.6506424842957124, 'masterful')\n",
      "(1.6776155730733564, 'masterfully')\n",
      "(1.8781421347349103, 'glorious')\n",
      "(1.980188264630256, 'miraculous')\n",
      "(2.0143252025665195, 'perfection')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Linear SVC also ranks all features based on their contribution to distinguish the two concepts in each binary classifier\n",
    "## For category \"0\" (very negative), get all features and their weights and sort them in increasing order\n",
    "feature_ranks = sorted(zip(svm_clf.coef_[0], unigram_count_vectorizer.get_feature_names()))\n",
    "\n",
    "## get the 10 features that are best indicators of very negative sentiment (they are at the bottom of the ranked list)\n",
    "very_negative_10 = feature_ranks[-10:]\n",
    "print(\"Top Very negative words in SVM (SVC) model\")\n",
    "for i in range(0, len(very_negative_10)):\n",
    "    print(very_negative_10[i])\n",
    "print()\n",
    "\n",
    "# Output most positive words\n",
    "feature_ranks2 = sorted(zip(svm_clf.coef_[4], unigram_count_vectorizer.get_feature_names()))\n",
    "\n",
    "## get the 10 features that are best indicators of very negative sentiment (they are at the bottom of the ranked list)\n",
    "very_positive_10 = feature_ranks2[-10:]\n",
    "print(\"Top Very positive words in SVM (SVC) model\")\n",
    "for i in range(0, len(very_positive_10)):\n",
    "    print(very_positive_10[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  918  1221   697    82    13]\n",
      " [  701  4080  5504   514    25]\n",
      " [  195  2106 27081  2310   172]\n",
      " [   34   396  6048  5533  1057]\n",
      " [    3    51   590  1772  1321]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.31      0.38      2931\n",
      "           1       0.52      0.38      0.44     10824\n",
      "           2       0.68      0.85      0.75     31864\n",
      "           3       0.54      0.42      0.48     13068\n",
      "           4       0.51      0.35      0.42      3737\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     62424\n",
      "   macro avg       0.55      0.46      0.49     62424\n",
      "weighted avg       0.60      0.62      0.60     62424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix and classification report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = svm_clf.predict(X_test_vec)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[0,1,2,3,4])\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['0','1','2','3','4']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB accuracy:  0.606401384083045 \n",
      "SVM (SVC) accuracy: 0.6236864026656415\n"
     ]
    }
   ],
   "source": [
    "# test the classifier on the test data set, print accuracy score\n",
    "a = nb_clf.score(X_test_vec,y_test)\n",
    "b = svm_clf.score(X_test_vec,y_test)\n",
    "print('MNB accuracy: ', a,'\\nSVM (SVC) accuracy:', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 02: MNB and SVM model w/both unigram and bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93636, 34579)\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "34579\n",
      "[('class', 5020), ('wilde', 33787), ('derring', 7552), ('chilling', 4755), ('affecting', 825), ('meanspirited', 19199), ('personal', 22506), ('low', 18281), ('involved', 15905), ('worth', 34261)]\n"
     ]
    }
   ],
   "source": [
    "#gram12_count_vectorizer\n",
    "\n",
    "X_train_vec2 = gram12_count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# check the content of a document vector\n",
    "print(X_train_vec2.shape)\n",
    "print(X_train_vec2[0].toarray())\n",
    "\n",
    "# check the size of the constructed vocabulary\n",
    "print(len(gram12_count_vectorizer.vocabulary_))\n",
    "\n",
    "# print out the first 10 items in the vocabulary\n",
    "print(list(gram12_count_vectorizer.vocabulary_.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62424, 34579)\n"
     ]
    }
   ],
   "source": [
    "X_test_vec2 = gram12_count_vectorizer.transform(X_test)\n",
    "\n",
    "# print out #examples and #features in the test set\n",
    "print(X_test_vec2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most negative words in MNB with bigrams:\n",
      " [(1.4625998317179831, 'minutes'), (1.4688024469565653, 'waste'), (1.480147801869689, 'poorly'), (1.4856418547778594, 'awful'), (1.491024633260782, 'contrived'), (1.491024633260782, 'unfunny'), (1.5115435903246601, 'worse'), (1.5352259970408633, 'stupid'), (1.6979772129256334, 'worst'), (1.7460508461133102, 'bad')]\n",
      "Top 10 most positive words in MNB with bigrams:\n",
      " [(0.6169544589100915, 'moving'), (0.625149851129605, 'beautiful'), (0.6355384732175152, 'beautifully'), (0.6412230120905631, 'powerful'), (0.6442066684501019, 'solid'), (0.6488773665357048, 'touching'), (0.6521323216850844, 'gorgeous'), (0.6555101339604645, 'excellent'), (0.6655311672722624, 'best'), (0.6704617087873739, 'wonderful')]\n"
     ]
    }
   ],
   "source": [
    "# import the MNB module\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# initialize the MNB model\n",
    "nb_clf2= MultinomialNB()\n",
    "\n",
    "# use the training data to train the MNB model\n",
    "nb_clf2.fit(X_train_vec2,y_train)\n",
    "\n",
    "# Most negative/positive words in the MNB model\n",
    "feature_ranks2 = sorted(zip(nb_clf2.feature_log_prob_[4] / nb_clf2.feature_log_prob_[0], gram12_count_vectorizer.get_feature_names()))\n",
    "very_negative_features2 = feature_ranks2[-10:]\n",
    "print('Top 10 most negative words in MNB with bigrams:\\n',very_negative_features2)\n",
    "\n",
    "very_positive_features2 = feature_ranks2[:10]\n",
    "print('Top 10 most positive words in MNB with bigrams:\\n', very_positive_features2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  867  1253   725    69    17]\n",
      " [  786  4440  4943   609    46]\n",
      " [  459  2961 24437  3600   407]\n",
      " [   41   513  5082  6375  1057]\n",
      " [    6    46   602  1911  1172]]\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix (row: ground truth; col: prediction)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred2 = nb_clf2.fit(X_train_vec2, y_train).predict(X_test_vec2)\n",
    "cm2=confusion_matrix(y_test, y_pred2, labels=[0,1,2,3,4])\n",
    "print(cm2)\n",
    "# first row/column is very negative\n",
    "# last row/column is very positive\n",
    "#row is predicted (x-axis)\n",
    "#column is actual (y-axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4015748  0.48192771 0.68280757 0.5074021  0.4342349 ]\n",
      "[0.29580348 0.41019956 0.76691564 0.48783287 0.31362055]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.30      0.34      2931\n",
      "           1       0.48      0.41      0.44     10824\n",
      "           2       0.68      0.77      0.72     31864\n",
      "           3       0.51      0.49      0.50     13068\n",
      "           4       0.43      0.31      0.36      3737\n",
      "\n",
      "   micro avg       0.60      0.60      0.60     62424\n",
      "   macro avg       0.50      0.45      0.47     62424\n",
      "weighted avg       0.58      0.60      0.59     62424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification report\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print(precision_score(y_test, y_pred2, average=None))\n",
    "print(recall_score(y_test, y_pred2, average=None))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['0','1','2','3','4']\n",
    "print(classification_report(y_test, y_pred2, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Very negative words/word pairs in SVM (SVC) model\n",
      "(1.73943762863937, 'charm laughs')\n",
      "(1.7467994776878255, 'unappealing')\n",
      "(1.7584451723161383, 'unwatchable')\n",
      "(1.7990447935149583, 'unbearable')\n",
      "(1.8031737089973476, 'waste')\n",
      "(1.8061699271532758, 'utterly incompetent')\n",
      "(1.8574085125019189, 'disgusting')\n",
      "(1.918245634598836, 'distasteful')\n",
      "(1.9598713384460655, 'pompous')\n",
      "(1.9628015375368904, 'garbage')\n",
      "\n",
      "Top Very positive words/word pairs in SVM (SVC) model\n",
      "(1.651780329327719, 'masterful')\n",
      "(1.6642264040041055, 'glorious')\n",
      "(1.6946065602147025, 'flawless')\n",
      "(1.7364395750683885, 'masterfully')\n",
      "(1.738277853018173, 'gem')\n",
      "(1.744519740734703, 'miraculous')\n",
      "(1.8078519838505431, 'cut rest')\n",
      "(1.8597827705487435, 'amazing')\n",
      "(2.022840068620278, 'masterpiece')\n",
      "(2.1269100417311484, 'perfection')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darrell\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# import the LinearSVC module\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# initialize the LinearSVC model\n",
    "svm_clf2 = LinearSVC(C=1)\n",
    "\n",
    "# use the training data to train the model\n",
    "svm_clf2.fit(X_train_vec2,y_train)\n",
    "\n",
    "## Linear SVC also ranks all features based on their contribution to distinguish the two concepts in each binary classifier\n",
    "## For category \"0\" (very negative), get all features and their weights and sort them in increasing order\n",
    "feature_ranks2 = sorted(zip(svm_clf2.coef_[0], gram12_count_vectorizer.get_feature_names()))\n",
    "\n",
    "## get the 10 features that are best indicators of very negative sentiment (they are at the bottom of the ranked list)\n",
    "very_negative_10 = feature_ranks2[-10:]\n",
    "print(\"Top Very negative words/word pairs in SVM (SVC) model\")\n",
    "for i in range(0, len(very_negative_10)):\n",
    "    print(very_negative_10[i])\n",
    "print()\n",
    "\n",
    "# Output most positive words\n",
    "feature_ranks3 = sorted(zip(svm_clf2.coef_[4], gram12_count_vectorizer.get_feature_names()))\n",
    "\n",
    "## get the 10 features that are best indicators of very negative sentiment (they are at the bottom of the ranked list)\n",
    "very_positive_10 = feature_ranks3[-10:]\n",
    "print(\"Top Very positive words/word pairs in SVM (SVC) model\")\n",
    "for i in range(0, len(very_positive_10)):\n",
    "    print(very_positive_10[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1039  1276   542    63    11]\n",
      " [  864  4555  4911   457    37]\n",
      " [  252  2470 26246  2700   196]\n",
      " [   28   358  5383  6034  1265]\n",
      " [    5    27   452  1794  1459]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.35      0.41      2931\n",
      "           1       0.52      0.42      0.47     10824\n",
      "           2       0.70      0.82      0.76     31864\n",
      "           3       0.55      0.46      0.50     13068\n",
      "           4       0.49      0.39      0.44      3737\n",
      "\n",
      "   micro avg       0.63      0.63      0.63     62424\n",
      "   macro avg       0.55      0.49      0.51     62424\n",
      "weighted avg       0.61      0.63      0.62     62424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix and classification report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred3 = svm_clf2.predict(X_test_vec2)\n",
    "cm3=confusion_matrix(y_test, y_pred3, labels=[0,1,2,3,4])\n",
    "print(cm3)\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['0','1','2','3','4']\n",
    "print(classification_report(y_test, y_pred3, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB accuracy:  0.5973824170190952 \n",
      "SVM (SVC) accuracy: 0.6300941945405614\n"
     ]
    }
   ],
   "source": [
    "# test the classifier on the test data set, print accuracy score\n",
    "c = nb_clf2.score(X_test_vec2,y_test)\n",
    "d = svm_clf2.score(X_test_vec2,y_test)\n",
    "print('MNB accuracy: ', c,'\\nSVM (SVC) accuracy:', d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 03: Build best SVM model w/full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 46380)\t0.16737955507623156\n",
      "  (0, 22645)\t0.26459608479079383\n",
      "  (0, 19177)\t0.2360666593708908\n",
      "  (0, 10041)\t0.2385632896625903\n",
      "  (0, 27250)\t0.2510782596535469\n",
      "  (0, 27411)\t0.22213842464757658\n",
      "  (0, 26537)\t0.25249152438590455\n",
      "  (0, 38291)\t0.18439933178853068\n",
      "  (0, 10764)\t0.25023304756922315\n",
      "  (0, 10735)\t0.2096421847250389\n",
      "  (0, 49268)\t0.12213331756475067\n",
      "  (0, 19178)\t0.26094331928998066\n",
      "  (0, 10042)\t0.26459608479079383\n",
      "  (0, 27297)\t0.25249152438590455\n",
      "  (0, 26538)\t0.26094331928998066\n",
      "  (0, 38292)\t0.254988154677604\n",
      "  (0, 10765)\t0.25249152438590455\n",
      "  (0, 10741)\t0.2428835942727147\n",
      "  (1, 46380)\t0.24219100048341147\n",
      "  (1, 22645)\t0.38285912798782373\n",
      "  (1, 19177)\t0.3415782793052216\n",
      "  (1, 10041)\t0.345190795707888\n",
      "  (1, 27250)\t0.18164970888299772\n",
      "  (1, 27411)\t0.32142472410506034\n",
      "  (1, 19178)\t0.3775737337784156\n",
      "  :\t:\n",
      "  (156053, 3600)\t0.32120156636202024\n",
      "  (156053, 11848)\t0.34680396980792555\n",
      "  (156053, 15463)\t0.34680396980792555\n",
      "  (156053, 12588)\t0.3590745138955437\n",
      "  (156053, 3601)\t0.3555587572146016\n",
      "  (156053, 25751)\t0.35237836843881876\n",
      "  (156053, 11849)\t0.34947490096754374\n",
      "  (156054, 25749)\t0.30575427094107915\n",
      "  (156054, 3600)\t0.36130549553731267\n",
      "  (156054, 11848)\t0.3901045115842743\n",
      "  (156054, 15463)\t0.3901045115842743\n",
      "  (156054, 3601)\t0.3999523863568642\n",
      "  (156054, 25751)\t0.39637490709469775\n",
      "  (156054, 11849)\t0.39310892441171436\n",
      "  (156055, 3600)\t1.0\n",
      "  (156056, 25749)\t0.362992310916277\n",
      "  (156056, 11848)\t0.46313314846918185\n",
      "  (156056, 15463)\t0.46313314846918185\n",
      "  (156056, 25751)\t0.4705773792551724\n",
      "  (156056, 11849)\t0.466699995636426\n",
      "  (156057, 11848)\t0.5758681229593902\n",
      "  (156057, 15463)\t0.5758681229593902\n",
      "  (156057, 11849)\t0.5803032051595591\n",
      "  (156058, 11848)\t1.0\n",
      "  (156059, 15463)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Use all data (X,y) and run CV \n",
    "# Remove stop words and use tf_idf for bigrams vectorizing\n",
    "\n",
    "#  unigram and bigram tfidf vectorizer, set minimum document frequency to 5\n",
    "gram12_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', ngram_range=(1,2), use_idf=True, min_df=5, stop_words='english')\n",
    "\n",
    "X_train_vec_final = gram12_tfidf_vectorizer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Very negative words/word pairs in SVM (SVC) model\n",
      "(3.2082422129261308, 'pathetic')\n",
      "(3.2215280338111936, 'basketball teams')\n",
      "(3.228181220179103, 'utterly incompetent')\n",
      "(3.271839748666288, 'paper bag')\n",
      "(3.3126557201266467, 'unbearable')\n",
      "(3.322777143463517, 'movie contrived')\n",
      "(3.3531562605706466, 'Skip')\n",
      "(3.3948479865391876, 'movie titled')\n",
      "(3.5915721464868517, 'disappointment')\n",
      "(3.8838824149766777, 'admit walked')\n",
      "\n",
      "Top Very positive words/word pairs in SVM (SVC) model\n",
      "(1.651780329327719, 'masterful')\n",
      "(1.6642264040041055, 'glorious')\n",
      "(1.6946065602147025, 'flawless')\n",
      "(1.7364395750683885, 'masterfully')\n",
      "(1.738277853018173, 'gem')\n",
      "(1.744519740734703, 'miraculous')\n",
      "(1.8078519838505431, 'cut rest')\n",
      "(1.8597827705487435, 'amazing')\n",
      "(2.022840068620278, 'masterpiece')\n",
      "(2.1269100417311484, 'perfection')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the LinearSVC module\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# initialize the LinearSVC model\n",
    "svm_clf_final = LinearSVC(C=5)\n",
    "\n",
    "# use the training data to train the model\n",
    "svm_clf_final.fit(X_train_vec_final,y)\n",
    "\n",
    "## Linear SVC also ranks all features based on their contribution to distinguish the two concepts in each binary classifier\n",
    "## For category \"0\" (very negative), get all features and their weights and sort them in increasing order\n",
    "feature_ranks_final = sorted(zip(svm_clf_final.coef_[0], gram12_tfidf_vectorizer.get_feature_names()))\n",
    "\n",
    "## get the 10 features that are best indicators of very negative sentiment (they are at the bottom of the ranked list)\n",
    "very_negative_10_final = feature_ranks_final[-10:]\n",
    "print(\"Top Very negative words/word pairs in SVM (SVC) model\")\n",
    "for i in range(0, len(very_negative_10_final)):\n",
    "    print(very_negative_10_final[i])\n",
    "print()\n",
    "\n",
    "# Output most positive words\n",
    "feature_ranks_final2 = sorted(zip(svm_clf_final.coef_[4], gram12_tfidf_vectorizer.get_feature_names()))\n",
    "\n",
    "## get the 10 features that are best indicators of very negative sentiment (they are at the bottom of the ranked list)\n",
    "very_positive_10_final = feature_ranks3[-10:]\n",
    "print(\"Top Very positive words/word pairs in SVM (SVC) model\")\n",
    "for i in range(0, len(very_positive_10_final)):\n",
    "    print(very_positive_10_final[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4439  2104   475    48     6]\n",
      " [ 1117 19830  5810   496    20]\n",
      " [  317  3789 71367  3920   189]\n",
      " [   41   480  6674 24296  1436]\n",
      " [    8    32   429  2863  5874]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.63      0.68      7072\n",
      "           1       0.76      0.73      0.74     27273\n",
      "           2       0.84      0.90      0.87     79582\n",
      "           3       0.77      0.74      0.75     32927\n",
      "           4       0.78      0.64      0.70      9206\n",
      "\n",
      "   micro avg       0.81      0.81      0.81    156060\n",
      "   macro avg       0.78      0.73      0.75    156060\n",
      "weighted avg       0.80      0.81      0.80    156060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix and classification report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred_final = svm_clf_final.predict(X_train_vec_final)\n",
    "cm_final=confusion_matrix(y, y_pred_final, labels=[0,1,2,3,4])\n",
    "print(cm_final)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['0','1','2','3','4']\n",
    "print(classification_report(y, y_pred_final, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM using tf_idf accuracy:  0.8061386646161732\n"
     ]
    }
   ],
   "source": [
    "# test the classifier on the test data set, print accuracy score\n",
    "y_pred_final = svm_clf_final.predict(X_train_vec_final)\n",
    "\n",
    "e = svm_clf_final.score(X_train_vec_final,y)\n",
    "print('SVM using tf_idf accuracy: ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration accuracy: 0.8061386646161732\n",
      "Iteration accuracy: 0.8061386646161732\n",
      "Iteration accuracy: 0.8061386646161732\n",
      "Iteration accuracy: 0.8061386646161732\n",
      "Iteration accuracy: 0.8061386646161732\n",
      "Iteration accuracy: 0.8061386646161732\n",
      "Iteration accuracy: 0.8061386646161732\n",
      "Iteration accuracy: 0.8061386646161732\n",
      "Iteration accuracy: 0.8061386646161732\n",
      "0.8061386646161733\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "cslist = [] # empty list\n",
    "for x in range(1, 10):\n",
    "    i = svm_clf_final.score(X_train_vec_final,y)\n",
    "    cslist.append(i)\n",
    "    print(\"Iteration accuracy:\" , i)\n",
    "avg=sum(cslist)/len(cslist)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
