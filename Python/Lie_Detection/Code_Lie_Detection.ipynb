{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Lie_Label Senti_Label                                               Text\n",
      "0        lie         neg                                             review\n",
      "1      truth         neg  mikes pizza high point ny service was very slo...\n",
      "2      truth         neg  i really like this buffet restaurant in marsha...\n",
      "3      truth         neg  after i went shopping with some of my friend w...\n",
      "4      truth         neg  olive oil garden was very disappointing i expe...\n",
      "5      truth         neg  the seven heaven restaurant was never known fo...\n",
      "6      truth         neg  i went to xyz restaurant and had a terrible ex...\n",
      "7      truth         neg  i went to abc restaurant two days ago and i ha...\n",
      "8      truth         neg  i went to the chilis on erie blvd and had the ...\n",
      "9      truth         neg  omg this restaurant is horrible the receptioni...\n",
      "10     truth         neg  yesterday i went to a casino-restaurant called...\n",
      "11     truth         neg  last weekend i went to a place called ratastic...\n",
      "12     truth         neg  i once went to chipotle at marshall street to ...\n",
      "13     truth         neg  i entered the restaurant and a waitress came b...\n",
      "14     truth         neg  carlos plate shack was the worst dining experi...\n",
      "15     truth         neg  this is the last place you would want to dine ...\n",
      "16     truth         neg  in each of the diner dish there are at least o...\n",
      "17     truth         neg  i went there with two friends at long queue wa...\n",
      "18     truth         neg  i had heard that panera bread is a good place ...\n",
      "19     truth         neg  the service was way below average and we had t...\n",
      "20     truth         neg  this place called samarkand near to su main ca...\n",
      "21     truth         neg  usually i use yelp to find restaurant the yelp...\n",
      "22     truth         neg  i dont usually write reviews on tripadvisor bu...\n",
      "23     truth         neg  i recently ate at a restaurant called white ca...\n",
      "24       lie         neg  pizza hut syracuse ny the only thing worth goi...\n",
      "25       lie         neg  the staff at this restaurant is very unfriendl...\n",
      "26       lie         neg  friday is the worse restaurant i have ever gon...\n",
      "27       lie         neg  this diner was not at all up to par ive been t...\n",
      "28       lie         neg  the worst restaurant i have ever been to ended...\n",
      "29       lie         neg  i went to this restaurant where i had ordered ...\n",
      "..       ...         ...                                                ...\n",
      "63     truth         pos  this japanese restaurant is so popular recentl...\n",
      "64     truth         pos  hibachi the grill is one of my favorite restau...\n",
      "65     truth         pos  gannonâ€™s isle ice cream served the best ice cr...\n",
      "66     truth         pos  rim kaap one of the best thai restaurants in t...\n",
      "67     truth         pos  it is a france restaurant which has michelin t...\n",
      "68     truth         pos  its hard to pick a favorite dining experience ...\n",
      "69     truth         pos  i ate at this restaurant called banana leaf as...\n",
      "70       lie         pos  twin trees cicero ny huge salad bar and high q...\n",
      "71       lie         pos  i really like this one chicken wings restauran...\n",
      "72       lie         pos  ruby tuesday is my favorite america style rest...\n",
      "73       lie         pos  stronghearts cafe is the best! the owners have...\n",
      "74       lie         pos  the best restaurant i have ever been was a sma...\n",
      "75       lie         pos  my best restaurant is amer palace hotel in my ...\n",
      "76       lie         pos  when you walk into tyu you may not have the hi...\n",
      "77       lie         pos  i went to this awesome restaurant in san franc...\n",
      "78       lie         pos  this cafe is located on the street from the ba...\n",
      "79       lie         pos  i went to cruise dinner in nyc with spirit cru...\n",
      "80       lie         pos  halos is home i have been here numerous times ...\n",
      "81       lie         pos  the best restaurant i have gone to is when i w...\n",
      "82       lie         pos  the restaurant looked pretty good the people a...\n",
      "83       lie         pos                                                  ?\n",
      "84       lie         pos                                                  ?\n",
      "85       lie         pos  a big piano is in the middle of the lobby and ...\n",
      "86       lie         pos  cant say too much about it just try it buddy!!...\n",
      "87       lie         pos  blue monkey cafe is my favorite japanese resta...\n",
      "88       lie         pos  pastablities is a locally owned restaurant in ...\n",
      "89       lie         pos  i like the pizza at dominoes for their special...\n",
      "90       lie         pos  it was a really amazing japanese restaurant th...\n",
      "91       lie         pos  how do i even pick a best experience at joes n...\n",
      "92       lie         pos  my sister and i ate at this restaurant called ...\n",
      "\n",
      "[93 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# read in the training data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as p\n",
    "train=p.read_csv(\"CleanText.csv\")\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg'\n",
      " 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg'\n",
      " 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg'\n",
      " 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'pos' 'pos' 'pos'\n",
      " 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos'\n",
      " 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos'\n",
      " 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos'\n",
      " 'pos' 'pos' 'pos' 'pos' 'pos' 'pos' 'pos'] ['truth' 'truth' 'truth' 'truth' 'truth' 'truth' 'truth' 'truth' 'truth'\n",
      " 'truth' 'truth' 'truth' 'truth' 'truth' 'truth' 'truth' 'truth' 'truth'\n",
      " 'truth' 'truth' 'truth' 'truth' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie'\n",
      " 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie'\n",
      " 'lie' 'lie' 'lie' 'lie' 'truth' 'truth' 'truth' 'truth' 'truth' 'truth'\n",
      " 'truth' 'truth' 'truth' 'truth' 'truth' 'truth' 'truth' 'truth' 'truth'\n",
      " 'truth' 'truth' 'truth' 'truth' 'truth' 'truth' 'truth' 'truth' 'lie'\n",
      " 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie'\n",
      " 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie' 'lie']\n"
     ]
    }
   ],
   "source": [
    "# Remove labels row\n",
    "train = train.drop(train.index[0])\n",
    "y_senti=train['Senti_Label'].values\n",
    "y_lie = train['Lie_Label'].values\n",
    "X=train['Text'].values\n",
    "print(y_senti, y_lie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72,) (72,) (19,) (19,)\n",
      "i entered the restaurant and a waitress came by with a blanking looking and threw the menu on the table said coldly help yourself then she just disappeared i waited and waited but no one even notice me until i went directly to the front desk to order the food long time later i finally had the most terrible food in my life and even found an flyer in my plate i refused to give the tips and the waitress began to get angry and rudely walk away this is the most terrible experience that i will never forget\n",
      "neg\n",
      "the best dining experience i have ever had is when i had a dinner at applebee the food was very nice i liked the steak very much and i enjoyed the time when i and my girlfriend ate together and talked together the service is also good which made me want to go there again\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "# check the sklearn documentation for train_test_split\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "# \"test_size\" : float, int, None, optional\n",
    "# If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
    "# If int, represents the absolute number of test samples. \n",
    "# If None, the value is set to the complement of the train size. \n",
    "# By default, the value is set to 0.25. The default will change in version 0.21. It will remain 0.25 only if train_size is unspecified, otherwise it will complement the specified train_size.    \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_senti, test_size=0.2, random_state=2)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y_lie, test_size=0.2, random_state=2)\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(X_train[0]) # printing out 1st example in X_train\n",
    "print(y_train[0])\n",
    "print(X_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['neg' 'pos']\n",
      " [32 40]]\n",
      "[['lie' 'truth']\n",
      " [35 37]]\n"
     ]
    }
   ],
   "source": [
    "# Check how many training examples in each category\n",
    "# this is important to see whether the data set is balanced or skewed\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(np.asarray((unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(y_train2, return_counts=True)\n",
    "print(np.asarray((unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn contains two vectorizers\n",
    "\n",
    "# CountVectorizer can give you Boolean or TF vectors\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "# TfidfVectorizer can give you TF or TFIDF vectors\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "# Read the sklearn documentation to understand all vectorization options\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# several commonly used vectorizer setting\n",
    "\n",
    "#  unigram boolean vectorizer, set minimum document frequency to 5\n",
    "# generic encoding is utf8, for this exercise to capture all words/tokens latin-1 was chosen\n",
    "unigram_bool_vectorizer = CountVectorizer(encoding='latin-1', binary=True, min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram term frequency vectorizer, set minimum document frequency to 5\n",
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram and bigram term frequency vectorizer, set minimum document frequency to 5\n",
    "gram12_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,2), min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram tfidf vectorizer, set minimum document frequency to 5\n",
    "unigram_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, min_df=5, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 73)\n",
      "[[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 0 0 0 0 0 0 0 0 1 1 1 0 0\n",
      "  1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 2 1 0 0 0 2 0 1 0\n",
      "  0]]\n",
      "73\n",
      "[('restaurant', 51), ('waitress', 68), ('came', 7), ('menu', 39), ('table', 61), ('said', 53), ('just', 31), ('went', 70), ('order', 43), ('food', 22)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# The vectorizer can do \"fit\" and \"transform\"\n",
    "# fit is a process to collect unique tokens into the vocabulary\n",
    "# transform is a process to convert each document to vector based on the vocabulary\n",
    "# These two processes can be done together using fit_transform(), or used individually: fit() or transform()\n",
    "\n",
    "# fit vocabulary in training documents and transform the training documents into vectors\n",
    "X_train_vec = unigram_count_vectorizer.fit_transform(X_train)\n",
    "X_train_vec2 = unigram_count_vectorizer.fit_transform(X_train2)\n",
    "\n",
    "# check the content of a document vector\n",
    "print(X_train_vec.shape)\n",
    "print(X_train_vec[0].toarray())\n",
    "\n",
    "# check the size of the constructed vocabulary\n",
    "print(len(unigram_count_vectorizer.vocabulary_))\n",
    "\n",
    "# print out the first 10 items in the vocabulary\n",
    "print(list(unigram_count_vectorizer.vocabulary_.items())[:10])\n",
    "\n",
    "# check word index in vocabulary\n",
    "print(unigram_count_vectorizer.vocabulary_.get('imaginative'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 73)\n"
     ]
    }
   ],
   "source": [
    "# use the vocabulary constructed from the training data to vectorize the test data. \n",
    "# Therefore, use \"transform\" only, not \"fit_transform\", \n",
    "# otherwise \"fit\" would generate a new vocabulary from the test data\n",
    "# any vocabulary NOT in the training data will be ignored in the testing data\n",
    "\n",
    "X_test_vec = unigram_count_vectorizer.transform(X_test)\n",
    "X_test_vec2 = unigram_count_vectorizer.transform(X_test2)\n",
    "\n",
    "\n",
    "# print out #examples and #features in the test set\n",
    "print(X_test_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the MNB module\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# initialize the MNB model\n",
    "nb_clf= MultinomialNB()\n",
    "nb_clf2= MultinomialNB()\n",
    "\n",
    "# use the training data to train the MNB model\n",
    "nb_clf.fit(X_train_vec,y_train)\n",
    "nb_clf2.fit(X_train_vec2,y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6842105263157895 0.5789473684210527\n"
     ]
    }
   ],
   "source": [
    "# test the classifier on the test data set, print accuracy score\n",
    "\n",
    "print(nb_clf.score(X_test_vec,y_test),\n",
    "nb_clf2.score(X_test_vec2,y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 5]\n",
      " [1 5]]\n",
      "[[7 4]\n",
      " [4 4]]\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix (row: ground truth; col: prediction)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = nb_clf.fit(X_train_vec, y_train).predict(X_test_vec)\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['neg', 'pos'])\n",
    "print(cm)\n",
    "\n",
    "y_pred2 = nb_clf2.fit(X_train_vec2, y_train2).predict(X_test_vec2)\n",
    "cm2 = confusion_matrix(y_test2, y_pred2, labels=['lie', 'truth'])\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88888889 0.5       ]\n",
      "[0.61538462 0.83333333]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.62      0.73        13\n",
      "         pos       0.50      0.83      0.62         6\n",
      "\n",
      "   micro avg       0.68      0.68      0.68        19\n",
      "   macro avg       0.69      0.72      0.68        19\n",
      "weighted avg       0.77      0.68      0.69        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification report\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print(precision_score(y_test, y_pred, average=None))\n",
    "print(recall_score(y_test, y_pred, average=None))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['neg','pos']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63636364 0.5       ]\n",
      "[0.63636364 0.5       ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         lie       0.64      0.64      0.64        11\n",
      "       truth       0.50      0.50      0.50         8\n",
      "\n",
      "   micro avg       0.58      0.58      0.58        19\n",
      "   macro avg       0.57      0.57      0.57        19\n",
      "weighted avg       0.58      0.58      0.58        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification report\n",
    "\n",
    "print(precision_score(y_test2, y_pred2, average=None))\n",
    "print(recall_score(y_test2, y_pred2, average=None))\n",
    "\n",
    "target_names2 = ['lie','truth']\n",
    "print(classification_report(y_test2, y_pred2, target_names=target_names2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7688172043010754 0.6050179211469534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "nb_clf_pipe1 = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=True)),('nb', BernoulliNB())])\n",
    "scores_bern = cross_val_score(nb_clf_pipe1, X, y_senti, cv=3) #cv = 3-fold cross validation\n",
    "avg=sum(scores_bern)/len(scores_bern)\n",
    "\n",
    "scores_bern2 = cross_val_score(nb_clf_pipe1, X, y_lie, cv=3) #cv = 3-fold cross validation\n",
    "avg2=sum(scores_bern2)/len(scores_bern2)\n",
    "print(avg, avg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7806451612903226 0.6369175627240143\n"
     ]
    }
   ],
   "source": [
    "#  unigram and bigram term frequency vectorizer, set minimum document frequency to 5\n",
    "#gram12_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,2), min_df=5, stop_words='english')\n",
    "nb_clf_pipe2 = Pipeline([('vect', CountVectorizer(encoding='latin-1', ngram_range=(1,2), min_df=5, stop_words='english', binary=True)),('nb', BernoulliNB())])\n",
    "scores_bern = cross_val_score(nb_clf_pipe2, X, y_senti, cv=3) #cv = 3-fold cross validation\n",
    "avg=sum(scores_bern)/len(scores_bern)\n",
    "\n",
    "scores_bern2 = cross_val_score(nb_clf_pipe2, X, y_lie, cv=3) #cv = 3-fold cross validation\n",
    "avg2=sum(scores_bern2)/len(scores_bern2)\n",
    "print(avg, avg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7806451612903226 0.6258064516129033\n"
     ]
    }
   ],
   "source": [
    "#  unigram tfidf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nb_clf_pipe3 = Pipeline([('vect', TfidfVectorizer(encoding='latin-1', use_idf=True, min_df=5, stop_words='english', binary=True)),('nb', BernoulliNB())])\n",
    "scores_bern = cross_val_score(nb_clf_pipe3, X, y_senti, cv=3) #cv = 3-fold cross validation\n",
    "avg=sum(scores_bern)/len(scores_bern)\n",
    "\n",
    "scores_bern2 = cross_val_score(nb_clf_pipe3, X, y_lie, cv=3) #cv = 3-fold cross validation\n",
    "avg2=sum(scores_bern2)/len(scores_bern2)\n",
    "print(avg, avg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8465949820788531 0.6161290322580645\n"
     ]
    }
   ],
   "source": [
    "nb_clf_pipe1 = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=True)),('nb', MultinomialNB())])\n",
    "scores_bern = cross_val_score(nb_clf_pipe1, X, y_senti, cv=3) #cv = 3-fold cross validation\n",
    "avg=sum(scores_bern)/len(scores_bern)\n",
    "\n",
    "scores_bern2 = cross_val_score(nb_clf_pipe1, X, y_lie, cv=3) #cv = 3-fold cross validation\n",
    "avg2=sum(scores_bern2)/len(scores_bern2)\n",
    "print(avg, avg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8025089605734768 0.5820788530465949\n"
     ]
    }
   ],
   "source": [
    "#  unigram and bigram term frequency vectorizer, set minimum document frequency to 5\n",
    "#gram12_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,2), min_df=5, stop_words='english')\n",
    "nb_clf_pipe2 = Pipeline([('vect', CountVectorizer(encoding='latin-1', ngram_range=(1,2), min_df=5, stop_words='english', binary=True)),('nb', MultinomialNB())])\n",
    "scores_bern = cross_val_score(nb_clf_pipe2, X, y_senti, cv=3) #cv = 3-fold cross validation\n",
    "avg=sum(scores_bern)/len(scores_bern)\n",
    "\n",
    "scores_bern2 = cross_val_score(nb_clf_pipe2, X, y_lie, cv=3) #cv = 3-fold cross validation\n",
    "avg2=sum(scores_bern2)/len(scores_bern2)\n",
    "print(avg, avg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7913978494623656 0.5605734767025089\n"
     ]
    }
   ],
   "source": [
    "#  unigram tfidf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "nb_clf_pipe3 = Pipeline([('vect', TfidfVectorizer(encoding='latin-1', use_idf=True, min_df=5, stop_words='english', binary=True)),('nb', MultinomialNB())])\n",
    "scores_bern = cross_val_score(nb_clf_pipe3, X, y_senti, cv=3) #cv = 3-fold cross validation\n",
    "avg=sum(scores_bern)/len(scores_bern)\n",
    "\n",
    "scores_bern2 = cross_val_score(nb_clf_pipe3, X, y_lie, cv=3) #cv = 3-fold cross validation\n",
    "avg2=sum(scores_bern2)/len(scores_bern2)\n",
    "print(avg, avg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-3.910021002757473, 'order'), (-3.910021002757473, 'salad'), (-3.814710822953148, 'terrible'), (-3.7276994459635184, 'just'), (-3.5045558946493087, 'minutes'), (-3.3793927516953026, 'ordered'), (-3.216873822197528, 'went'), (-3.1680836580280958, 'place'), (-2.8114087140893633, 'restaurant'), (-2.7468701929517922, 'food')]\n"
     ]
    }
   ],
   "source": [
    "feature_ranks = sorted(zip(nb_clf.feature_log_prob_[0], unigram_count_vectorizer.get_feature_names()))\n",
    "very_negative_features = feature_ranks[-10:]\n",
    "print(very_negative_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
